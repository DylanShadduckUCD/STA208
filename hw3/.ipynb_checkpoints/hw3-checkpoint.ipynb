{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 208: Homework 3 (Do not distribute)\n",
    "\n",
    "__Instructions:__ Submit it on canvas.  The canvas should include all of your code either in this notebook file, or a separate python file that is imported and ran in this notebook.  We should be able to open this notebook and run everything here by running the cells in sequence.  The written portions can be either done in markdown and TeX in new cells or written clearly by hand when you hand it in.  Submit each file separately.\n",
    "\n",
    "- Code should be well organized and documented\n",
    "- All math should be clear and make sense sequentially\n",
    "- When in doubt explain what is going on\n",
    "- You will be graded on correctness of your math, code efficiency and succinctness, and conclusions and modelling decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__ (10 pts)\n",
    "\n",
    "Recall that surrogate losses for large margin classification take the form, $\\phi(y_i x_i^\\top \\beta)$ where $y_i \\in \\{-1,1\\}$ and $\\beta, x_i \\in \\mathbb R^p$.\n",
    "\n",
    "The following functions are used as surrogate losses for large margin classification.  Demonstrate if they are convex or not, and follow the instructions.\n",
    "\n",
    "1. exponential loss: $\\phi(x) = e^{-x}$\n",
    "1. truncated quadratic loss: $\\phi(x) = (\\max\\{1-x,0\\})^2$\n",
    "1. hinge loss: $\\phi(x) = \\max\\{1-x,0\\}$\n",
    "1. sigmoid loss: $\\phi(x) = 1 - \\tanh(\\kappa x)$, for fixed $\\kappa > 0$\n",
    "1. Plot these as a function of $x$.\n",
    "\n",
    "(This problem is due to notes of Larry Wasserman.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__ (20 pts)\n",
    "\n",
    "Consider the truncated quadratic loss from (1.1.2).  For brevity let $a_+ = max\\{a,0\\}$ denote the positive part of $a$.\n",
    "\n",
    "$$\\ell(y_i,x_i,\\beta) = \\phi(y_i x_i^\\top \\beta) = (1-y_i x_i^\\top \\beta)_+^2$$\n",
    "\n",
    "1. Consider the empirical risk, $R_n$ (the average loss over a training set) for the truncated quadratic loss.  What is gradient of $R_n$ in $\\beta$?  Does it always exists?\n",
    "1. Demonstrate that the gradient does not have continuous derivative everywhere.\n",
    "1. Recall that support vector machines used the hinge loss $(1 - y_i x_i^\\top \\beta)_+$ with a ridge regularization.  Write the regularized optimization method for the truncated quadratic loss, and derive the gradient of the regularized empirical risk.\n",
    "1. In quasi-Newton methods a matrix ($Q$) that is a surrogate for the Hessian of the objective $L$ is used to determine step direction.\n",
    "\n",
    "$$\n",
    "\\beta \\gets \\beta - Q^{-1} \\nabla L(\\beta)\n",
    "$$\n",
    "\n",
    "\n",
    "Because the loss does not have continuous Hessian, instead of the Newton method, we will use a quasi-Newton method that replaces the Hessian with a quasi-Hessian (another matrix that is meant to approximate the Hessian).  Consider the following quasi-Hessian of the regularized objective to be $$G(\\beta) = \\frac 1n \\sum_i 2 (x_i x_i^\\top 1\\{ y_i x_i^\\top \\beta < 1 \\}) + 2 \\lambda I.$$  Demonstrate that the quasi-Hessian is positive definite, and write pseudo-code for quasi-Newton optimization, comment on the computational complexity of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 (20 pts)**\n",
    "\n",
    "Consider the simulation below.\n",
    "\n",
    "1. Implement minibatch stochastic gradient descent with the truncated quadratic loss using just numpy and base python.  Access the data by iteratively calling the ``sim_data`` method below.  \n",
    "\n",
    "2. With minibatch size of $1$ (SGD).  Vary the learning schedule to be constant, decaying with $\\eta_t \\propto t^{-1/2}$, and $\\eta_t \\propto t^{-1}$.  Compare these schedules using normal noise (the ``noise_dis`` parm).\n",
    "\n",
    "3. Vary the minibatch size to see the change in performance, with the best learning schedule from 2. When you compare two methods, make sure that you compare them with the same amount of data accessed (so use 1:10 ratio of iterations if you are comparing a minibatch ratio of 10:1).\n",
    "\n",
    "4. Redo 2, 3 with ``noise_dis`` set to ``\"chisquare\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSimulator:\n",
    "    \"\"\"\n",
    "    Simulate the data for linear classification\n",
    "    \"\"\"\n",
    "    def __init__(self,p,noise_dist = \"normal\"):\n",
    "        self.beta = np.random.normal(0,1,p)\n",
    "        self.noise_dist = noise_dist\n",
    "        self.p = p\n",
    "        \n",
    "    def sim_data(self,m = 1):\n",
    "        p = self.p\n",
    "        X = np.random.normal(0,1,(m,p))\n",
    "        if self.noise_dist == \"normal\":\n",
    "            eps = np.random.normal(0,1,m)\n",
    "        if self.noise_dist == \"chisquare\":\n",
    "            eps = np.random.chisquare(1,m)\n",
    "        z = X @ self.beta + eps\n",
    "        y = 2*(z > 0)-1\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.17459256, -0.5950664 ,  0.14164875,  1.00789915,  1.52097789,\n",
       "         -2.35292289, -0.50395825, -0.2364724 ,  1.05704893, -0.85057724],\n",
       "        [ 2.24071163, -0.55677462, -0.87292496, -2.01811478,  0.31402302,\n",
       "          1.08389045, -1.04015489, -0.4591674 , -0.84599752, -2.03291433],\n",
       "        [ 0.65142912,  0.51700884, -0.56405856,  0.25018309,  0.27288681,\n",
       "         -2.39893734, -1.26470106,  0.08149991, -0.2103138 , -1.92803075],\n",
       "        [-1.07546376, -0.12527754, -0.04904566,  0.63236309,  0.16898363,\n",
       "         -0.49714911, -0.07328165,  1.09958584,  0.30888407, -0.76970409],\n",
       "        [ 1.91230717,  0.67838713, -0.97439855, -0.72950593, -0.50834705,\n",
       "         -0.34413199, -0.40604259, -0.89565974,  1.17803265,  0.80007911],\n",
       "        [-0.91934717, -1.15178371, -1.63185586,  0.4312865 ,  0.08805506,\n",
       "         -0.78994413, -0.33735674, -0.85196559, -0.63270512, -0.01627497],\n",
       "        [ 0.36672325,  0.87293361, -0.62322003, -0.72003159, -0.18015153,\n",
       "          0.01737342, -0.25650232,  1.46494506,  1.04346338, -1.32347139],\n",
       "        [ 1.80499915,  0.1547483 ,  0.09121837,  2.04685017,  0.7384507 ,\n",
       "         -0.82035118,  1.33205988, -0.70850913,  0.88393086,  0.71329973],\n",
       "        [ 1.04309378, -0.56249175,  1.52847301,  0.76357975,  2.1522095 ,\n",
       "          1.28985151,  1.10218112,  0.0510469 , -1.2897831 , -0.88215084],\n",
       "        [-0.63163572,  0.77518644, -2.24288169,  0.44228795, -0.15852758,\n",
       "         -0.21390398,  0.23289109, -3.02033458,  0.299431  ,  0.27776615]]),\n",
       " array([-1, -1,  1,  1,  1,  1,  1,  1, -1,  1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DataSimulator(10)\n",
    "ds.sim_data(m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(ds,T,m,beta_start,C,gamma):\n",
    "    \"\"\"\n",
    "    ds: data simulator\n",
    "    T: number of iterations\n",
    "    m: minibatch size\n",
    "    beta_start: init beta\n",
    "    C, gamma : eta_t = C t^(-gamma)\n",
    "    \"\"\"\n",
    "    losses = [] # tracks loss at each time\n",
    "    betas = [] # tracks betas at each time\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return(betas,losses)\n",
    "\n",
    "perf = [np.sum((beta- beta_true)**2) for beta in betas]\n",
    "plt.plot(np.arange(T)*m, perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.__ (50 pts) \n",
    "\n",
    "Text data can be converted into vector data through a vectorization operation.  A corpus is a collection of documents and the dictionary is all of the words in the corpus.  Bag-of-words models will treat each document as a set of words, ignoring the order of the words.  Then a simple vectorizer will let $X_{i,j}$ be the number of times the $j$th word is in the $i$th document.  Two vectorizers are ``sklearn.feature_extraction.text.CountVectorizer`` and ``sklearn.feature_extraction.text.TfidfVectorizer``.\n",
    "\n",
    "Below is an import of a reuters dataset.  I have written a def to process a single file.  Construct a response variable that has three categories, if the topic is 'earn', 'acq', or another category.  Import all of the data and construct two sparse vectorized matrices---look at ``scipy.sparse``---based on the two above vectorizations.  Use sklearn svm.SVC on the TRAIN split and predict on the TEST split.  Plot your ROC and PR curves for predicting 'earn' (versus everything else); use the linear kernel and tune the C parameter.  Do the same for predicting 'acq' versus everything else.  Write a paragraph summarizing the performance and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html, etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reu = html.parse(\"reuters/reut2-000.sgm\") #You will have to do this for all sgm files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "# Download Corpora -> stopwords, Models -> punkt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reu(reu):\n",
    "    \"\"\"Parses the etree object and returns a list of dictionary of reuters attr\n",
    "    Output: {'topics': the topic of the article, 'places': where it is located, \n",
    "        'split': training/test split, 'body':the text of the article as a set of words with stopwords removed}\n",
    "    \"\"\"\n",
    "    root= reu.getroot()\n",
    "    articles = root.body.getchildren()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    reu_pl = []\n",
    "    for a in articles:\n",
    "        reu_parse = {}\n",
    "        if a.attrib['topics'] != 'YES':\n",
    "            next\n",
    "        topics = a.find('topics').findall('d')\n",
    "        if topics:\n",
    "            reu_parse['topics'] = [t.text for t in topics]\n",
    "        else:\n",
    "            reu_parse['topics'] = []\n",
    "        places = a.find('places').findall('d')\n",
    "        if places:\n",
    "            reu_parse['places'] = [t.text for t in places]\n",
    "        reu_parse['split'] = a.attrib['lewissplit']\n",
    "        rtxt = a.find('text')\n",
    "        word_tokens = word_tokenize(rtxt.text_content())\n",
    "        filtered_sentence = set([w.lower() for w in word_tokens if not w in stop_words])\n",
    "        reu_parse['body'] = filtered_sentence\n",
    "        reu_pl.append(reu_parse)\n",
    "    return reu_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reu_pl = parse_reu(reu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cocoa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'currency currently stage fit april/may quality although spot +bahia 2,325 reuter doubts also light normal argentina year 350 going 27 26 available processors 1,880 1,870 alleviating july continued went weekly 2,380 covertible selling 995 nearby bags dec cumulative 2.27 oct/dec convertible would still prices 1.25 shippers 15 drought again limited 22 ports 6.13 bahia january review farmers buyers 4,450 making rose march/april coming may much end included 1.06 6.4 june/july ended u.s. published good 785 harvesting certificates fob - 1,850 carnival throughout early 1,875 comissaria shipment uruguay part reluctant 28 6.2 middlemen salvador superior+ 340 lower held registered estimated improving 4,400 york times dry 2,400 week levels arrivals booked 4,340 come trade showers mln dificulties almost march commission feb practically sept 155,221 doubt 5.81 smith estimates 0.39 1986/87 5.93 ends 2.28 4,480 obtaining temporao february around cocoa standing crop late said recent weeks 45 liquor there total new export prospects cruzados 60 cake thousand consignment sales 4,345 the made destinations view 4,415 named 753 aug midday arroba seems 1,780 4,351 brazilian last offer restored aug/sept areas open butter tonne exporters dlrs final expected zone kilos bean humidity means per 1,750 2,375 old season experiencing hundred hands , in delivered 4,350 sold since 35 figures earlier routine period . 1987/88 with'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reu_pl[0]['topics'])\n",
    "\" \".join(reu_pl[0]['body'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
